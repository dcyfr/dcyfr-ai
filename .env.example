# ============================================
# DCYFR AI - Memory Layer Configuration
# ============================================
# Copy this file to .env and fill in your values

# ============================================
# Vector Database Configuration
# ============================================

# Vector DB provider: 'qdrant' | 'pinecone' | 'weaviate'
VECTOR_DB_PROVIDER=qdrant

# For Qdrant (local development)
VECTOR_DB_URL=http://localhost:6333

# For Pinecone (production)
# VECTOR_DB_API_KEY=your-pinecone-api-key
# VECTOR_DB_ENVIRONMENT=us-west1-gcp

# For Authenticated Qdrant (cloud)
# VECTOR_DB_API_KEY=your-qdrant-api-key

# Collection/Index name (default: dcyfr_memories)
VECTOR_DB_INDEX=dcyfr_memories

# ============================================
# LLM Configuration (for embeddings)
# ============================================

# LLM provider: 'openai' | 'anthropic' | 'custom'
LLM_PROVIDER=openai

# ============================================
# LLM Provider Options
# ============================================

# --- Option 1: OpenAI Direct ---
# OpenAI API Key (used if LLM_PROVIDER=openai)
OPENAI_API_KEY=your-openai-api-key

# --- Option 2: Anthropic Direct ---
# Anthropic API Key (used if LLM_PROVIDER=anthropic)
# ANTHROPIC_API_KEY=your-anthropic-api-key

# --- Option 3: Msty Studio Vibe CLI Proxy ---
# OpenAI-compatible endpoint for local model routing
# Supports: Claude Code, OpenAI Codex, GitHub Copilot, Google Gemini, Qwen Code, iFlow, Antigravity
# Docs: https://docs.msty.studio/features/vibe-cli-proxy
# OPENAI_API_BASE=http://localhost:8317/
# OPENAI_API_KEY=msty-vibe-proxy  # Any non-empty string for local proxy

# --- Option 4: Ollama (Local) ---
# Local Ollama instance for offline development
# OLLAMA_URL=http://localhost:11434

# --- Option 5: Custom LLM Provider ---
# Custom LLM API endpoint and key
# LLM_API_KEY=your-llm-api-key
# LLM_API_BASE=https://your-custom-endpoint.com/v1

# ============================================
# Model Configuration
# ============================================

# Model for memory operations
LLM_MODEL=gpt-4

# Embedding model (critical for vector search)
LLM_EMBEDDING_MODEL=text-embedding-3-small

# ============================================
# Memory Caching Configuration
# ============================================

# Enable caching (reduces duplicate vector searches)
MEMORY_CACHE_ENABLED=true

# Cache TTL in seconds (default: 300 = 5 minutes)
MEMORY_CACHE_TTL=300

# Max cache entries (default: 1000)
MEMORY_CACHE_MAX_SIZE=1000

# ============================================
# Telemetry & Analytics (Optional)
# ============================================

# Upstash Redis for production telemetry storage
# Get credentials from: https://console.upstash.com/
# UPSTASH_REDIS_REST_URL=https://your-upstash-url.upstash.io
# UPSTASH_REDIS_REST_TOKEN=your-upstash-token

# Mem0 API for advanced memory features
# Get API key from: https://mem0.ai/
# MEM0_API_KEY=your-mem0-api-key

# ============================================
# Development & Testing
# ============================================

# Node environment
NODE_ENV=development

# Enable debug logging
# DEBUG=dcyfr:*

# Version detection (for compatibility checks)
DCYFR_AGENTS_VERSION=1.0.0

# ============================================
# Development Notes
# ============================================
# 
# Local Development (Qdrant):
# 1. Start Qdrant: docker compose up -d qdrant
# 2. Set VECTOR_DB_PROVIDER=qdrant
# 3. Set VECTOR_DB_URL=http://localhost:6333
#
# Production (Pinecone):
# 1. Create Pinecone account: https://www.pinecone.io/
# 2. Set VECTOR_DB_PROVIDER=pinecone
# 3. Set VECTOR_DB_API_KEY + VECTOR_DB_ENVIRONMENT
#
# Msty Studio Vibe CLI Proxy (Multi-Model Local Routing):
# 1. Start Msty Studio and enable Vibe CLI Proxy (default: http://localhost:8317)
# 2. Set OPENAI_API_BASE=http://localhost:8317/v1
# 3. Set OPENAI_API_KEY=msty-vibe-proxy (any non-empty value)
# 4. Set LLM_PROVIDER=openai (uses OpenAI-compatible endpoint)
# 5. Choose model: gpt-4, claude-3-5-sonnet, copilot-gpt-4, gemini-pro, etc.
# 6. Vibe CLI Proxy routes requests to appropriate provider automatically
# Note: Supports Claude Code, OpenAI Codex, GitHub Copilot, Google Gemini,
#       Qwen Code, iFlow, and Antigravity models
# Docs: https://docs.msty.studio/features/vibe-cli-proxy
#
# ============================================
